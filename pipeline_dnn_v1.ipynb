{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Power Flow Neural Network Training with Pandapower\n",
    "\n",
    "This notebook demonstrates how to create a pipeline for training a neural network to learn power flow solutions using pandapower's internal states."
   ],
   "id": "e1e9573c2e35c44e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:30:36.793326Z",
     "start_time": "2025-01-21T14:30:26.828344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandapower as pp\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter # for pytorch visualization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # normalize input features and target values\n",
    "from sklearn.model_selection import ParameterGrid # for hyperparameter tuning\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n"
   ],
   "id": "d7e29ffe2e7920de",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Import Data Set\n",
    "\n",
    "First, we import and preprocess a PyTorch dataset class that interfaces with pandapower:"
   ],
   "id": "1c544dc9ac3f11ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T14:35:35.365251Z",
     "start_time": "2025-01-21T14:35:35.326547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = np.load('grid_dataset/vector_data.npy')\n",
    "print(data)\n"
   ],
   "id": "31f12038d1d4aaf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   1.13288357e+00  8.82832147e-01 -1.74707942e+01 -1.41440771e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   9.26709859e-01  1.13721556e+00 -2.23383005e+01 -1.50719267e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   9.81499345e-01  9.11973645e-01 -9.38339418e+00 -1.25780790e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   1.13345815e+00  1.06089334e+00  9.18156678e+00 -1.46048477e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   9.87256246e-01  9.36622251e-01  1.63211268e+01 -1.40554228e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   1.10766661e+00  1.08437289e+00 -9.33994642e+00 -1.55972207e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   9.35332856e-01  9.47932841e-01  4.73564092e+00 -1.43092819e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   1.07681449e+00  9.18750282e-01 -9.51835908e+00 -1.61627943e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   9.34569650e-01  1.11875148e+00 -1.67191011e+01 -1.53294713e+02\n",
      "   3.00000000e+00]\n",
      " [ 0.00000000e+00 -1.00000000e-01  0.00000000e+00 -5.00000000e-02\n",
      "   1.00020000e+00 -1.15659131e+00  2.88829571e+00  1.00020000e+00\n",
      "  -4.04488704e+00 -4.00287492e+00 -3.00307492e+00 -4.04488704e+00\n",
      "   1.11759154e+00  8.58391329e-01 -2.85646530e+01 -1.26356141e+02\n",
      "   3.00000000e+00]]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Create Internal Model\n",
    "\n",
    "Next, we define our internal model architecture:"
   ],
   "id": "b60fb10ff26e6c08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class InternalModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(InternalModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "f4b780979cebdbf7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training Pipeline\n",
    "\n",
    "Create the training function:"
   ],
   "id": "c917333691c8b907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_power_flow_model(base_network, num_epochs=100, batch_size=32):\n",
    "    # Create dataset\n",
    "    dataset = #insert dataset\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_size = len(dataset[0]['input'])\n",
    "    output_size = len(dataset[0]['output'])\n",
    "    model = PowerFlowDNN(input_size=input_size, hidden_size=512, output_size=output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch_inputs = batch['input']\n",
    "            batch_targets = batch['output']\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch_inputs = batch['input']\n",
    "                batch_targets = batch['output']\n",
    "                outputs = model(batch_inputs)\n",
    "                loss = criterion(outputs, batch_targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Log training and validation loss to TensorBoard\n",
    "        writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    writer.close()\n",
    "    return model"
   ],
   "id": "bed112667f7a0282"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Usage Example\n",
    "\n",
    "Here's how to use the trained model:"
   ],
   "id": "8462f6bba0c0d845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_power_flow(model, net):\n",
    "    # Run power flow to ensure internal data is available\n",
    "    pp.runpp(net, calculate_voltage_angles=True)\n",
    "    \n",
    "    # Prepare input\n",
    "    Ybus = net._ppc[\"internal\"][\"Ybus\"].toarray()\n",
    "    S = net._ppc[\"internal\"][\"Sbus\"]\n",
    "    input_tensor = torch.FloatTensor(np.concatenate([\n",
    "        Ybus.real.flatten(), \n",
    "        Ybus.imag.flatten(),\n",
    "        S.real, \n",
    "        S.imag\n",
    "    ]))\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    # Split prediction into voltage magnitudes and angles\n",
    "    n_buses = len(net.bus)\n",
    "    V_mag_pred = output[:n_buses].numpy()\n",
    "    V_ang_pred = output[n_buses:].numpy()\n",
    "    \n",
    "    # Get reference values from the network\n",
    "    V_mag_ref = net.res_bus.vm_pu.values\n",
    "    V_ang_ref = net.res_bus.va_degree.values\n",
    "    \n",
    "    return V_mag_pred, V_ang_pred, V_mag_ref, V_ang_ref"
   ],
   "id": "ec720f92675663a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Example Usage\n",
    "\n",
    "Here's how to put it all together:"
   ],
   "id": "4dc6d21bb46e7388"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a simple test network\n",
    "# net = pp.create_empty_network()\n",
    "# Add your network elements here...\n",
    "net = pp.networks.example_simple()\n",
    "\n",
    "# Train the model\n",
    "model = train_power_flow_model(net)\n",
    "\n",
    "# Make predictions\n",
    "V_mag_pred, V_ang_pred, V_mag_ref, V_ang_ref = predict_power_flow(model, net)\n",
    "print(\"Predicted voltage magnitudes:\", V_mag_pred)\n",
    "print(\"Predicted voltage angles:\", V_ang_pred)\n",
    "print(\"Reference voltage magnitudes:\", V_mag_ref)\n",
    "print(\"Reference voltage angles:\", V_ang_ref)"
   ],
   "id": "7ebafab7d5db34b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Additional Hyperparameter Tuning\n",
   "id": "8750e487a27f2cbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch_inputs = batch['input']\n",
    "            batch_targets = batch['output']\n",
    "            outputs = model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    return val_loss\n",
    "\n",
    "def hyperparameter_tuning(base_network, param_grid):\n",
    "    best_model = None\n",
    "    best_loss = float('inf')\n",
    "    dataset = PowerFlowDataset(base_network)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    _, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(f\"Training with parameters: {params}\")\n",
    "        model = train_power_flow_model(base_network, num_epochs=params['num_epochs'], batch_size=params['batch_size'])\n",
    "        val_loss = evaluate_model(model, val_loader, nn.MSELoss())\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "    return best_model\n",
    "\n",
    "base_network = pp.networks.example_simple()\n",
    "param_grid = {\n",
    "    'num_epochs': [50, 100],\n",
    "    'batch_size': [16, 32],\n",
    "    'hidden_size': [256, 512]\n",
    "}\n",
    "best_model = hyperparameter_tuning(base_network, param_grid)\n",
    "# ..."
   ],
   "id": "2045875b3bbdec1c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
